++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ '[' -z /opt/java/openjdk ']'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
++ command -v readarray
+ '[' readarray ']'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -z ']'
+ '[' -z ']'
+ '[' -n '' ']'
+ '[' -z ']'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*'
+ SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*:/opt/spark/work-dir'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --conf "spark.executorEnv.SPARK_DRIVER_POD_IP=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.224.118.142 --conf spark.executorEnv.SPARK_DRIVER_POD_IP=10.224.118.142 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///mounts/shared-volume/shared/harshal/sparkconsumerbasic.py
Files local:///mounts/shared-volume/shared/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar from /mounts/shared-volume/shared/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar to /opt/spark/work-dir/spark-sql-kafka-0-10_2.12-3.5.1.jar
Files local:///mounts/shared-volume/shared/jars/kafka-clients-2.6.1.jar from /mounts/shared-volume/shared/jars/kafka-clients-2.6.1.jar to /opt/spark/work-dir/kafka-clients-2.6.1.jar
Files local:///mounts/shared-volume/shared/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar from /mounts/shared-volume/shared/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /opt/spark/work-dir/spark-token-provider-kafka-0-10_2.12-3.5.1.jar
Files local:///mounts/shared-volume/shared/jars/commons-pool2-2.11.1.jar from /mounts/shared-volume/shared/jars/commons-pool2-2.11.1.jar to /opt/spark/work-dir/commons-pool2-2.11.1.jar
24/09/06 08:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
/opt/spark/work-dir
['..2024_09_06_08_11_17.643943941', '..data', 'spark.properties']
['HikariCP-2.5.1.jar', 'JLargeArrays-1.5.jar', 'JTransforms-3.1.jar', 'RoaringBitmap-0.9.45.jar', 'ST4-4.0.4.jar', 'activation-1.1.1.jar', 'aircompressor-0.26.jar', 'algebra_2.12-2.0.1.jar', 'aliyun-java-sdk-core-4.5.10.jar', 'aliyun-java-sdk-kms-2.11.0.jar', 'aliyun-java-sdk-ram-3.1.0.jar', 'aliyun-sdk-oss-3.13.0.jar', 'annotations-17.0.0.jar', 'antlr-runtime-3.5.2.jar', 'antlr4-runtime-4.9.3.jar', 'aopalliance-repackaged-2.6.1.jar', 'arpack-3.0.3.jar', 'arpack_combined_all-0.1.jar', 'arrow-format-12.0.1.jar', 'arrow-memory-core-12.0.1.jar', 'arrow-memory-netty-12.0.1.jar', 'arrow-vector-12.0.1.jar', 'audience-annotations-0.5.0.jar', 'avro-1.11.2.jar', 'avro-ipc-1.11.2.jar', 'avro-mapred-1.11.2.jar', 'aws-java-sdk-bundle-1.12.262.jar', 'azure-data-lake-store-sdk-2.3.9.jar', 'azure-keyvault-core-1.0.0.jar', 'azure-storage-7.0.1.jar', 'blas-3.0.3.jar', 'bonecp-0.8.0.RELEASE.jar', 'breeze-macros_2.12-2.1.0.jar', 'breeze_2.12-2.1.0.jar', 'cats-kernel_2.12-2.1.1.jar', 'chill-java-0.10.0.jar', 'chill_2.12-0.10.0.jar', 'commons-cli-1.5.0.jar', 'commons-codec-1.16.0.jar', 'commons-collections-3.2.2.jar', 'commons-collections4-4.4.jar', 'commons-compiler-3.1.9.jar', 'commons-compress-1.23.0.jar', 'commons-crypto-1.1.0.jar', 'commons-dbcp-1.4.jar', 'commons-io-2.13.0.jar', 'commons-lang-2.6.jar', 'commons-lang3-3.12.0.jar', 'commons-logging-1.1.3.jar', 'commons-math3-3.6.1.jar', 'commons-pool-1.5.4.jar', 'commons-text-1.10.0.jar', 'compress-lzf-1.1.2.jar', 'curator-client-2.13.0.jar', 'curator-framework-2.13.0.jar', 'curator-recipes-2.13.0.jar', 'datanucleus-api-jdo-4.2.4.jar', 'datanucleus-core-4.1.17.jar', 'datanucleus-rdbms-4.1.19.jar', 'datasketches-java-3.3.0.jar', 'datasketches-memory-2.1.0.jar', 'derby-10.14.2.0.jar', 'dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar', 'flatbuffers-java-1.12.0.jar', 'gcs-connector-hadoop3-2.2.14-shaded.jar', 'gson-2.2.4.jar', 'guava-14.0.1.jar', 'hadoop-aliyun-3.3.4.jar', 'hadoop-annotations-3.3.4.jar', 'hadoop-aws-3.3.4.jar', 'hadoop-azure-3.3.4.jar', 'hadoop-azure-datalake-3.3.4.jar', 'hadoop-client-api-3.3.4.jar', 'hadoop-client-runtime-3.3.4.jar', 'hadoop-cloud-storage-3.3.4.jar', 'hadoop-openstack-3.3.4.jar', 'hadoop-shaded-guava-1.1.1.jar', 'hive-common-2.3.9.jar', 'hive-exec-2.3.9-core.jar', 'hive-llap-common-2.3.9.jar', 'hive-metastore-2.3.9.jar', 'hive-serde-2.3.9.jar', 'hive-shims-0.23-2.3.9.jar', 'hive-shims-2.3.9.jar', 'hive-shims-common-2.3.9.jar', 'hive-shims-scheduler-2.3.9.jar', 'hive-storage-api-2.8.1.jar', 'hk2-api-2.6.1.jar', 'hk2-locator-2.6.1.jar', 'hk2-utils-2.6.1.jar', 'httpclient-4.5.14.jar', 'httpcore-4.4.16.jar', 'ini4j-0.5.4.jar', 'istack-commons-runtime-3.0.8.jar', 'ivy-2.5.1.jar', 'jackson-annotations-2.15.2.jar', 'jackson-core-2.15.2.jar', 'jackson-core-asl-1.9.13.jar', 'jackson-databind-2.15.2.jar', 'jackson-dataformat-cbor-2.15.2.jar', 'jackson-dataformat-yaml-2.15.2.jar', 'jackson-datatype-jsr310-2.15.2.jar', 'jackson-mapper-asl-1.9.13.jar', 'jackson-module-scala_2.12-2.15.2.jar', 'jakarta.annotation-api-1.3.5.jar', 'jakarta.inject-2.6.1.jar', 'jakarta.servlet-api-4.0.3.jar', 'jakarta.validation-api-2.0.2.jar', 'jakarta.ws.rs-api-2.1.6.jar', 'jakarta.xml.bind-api-2.3.2.jar', 'janino-3.1.9.jar', 'javassist-3.29.2-GA.jar', 'javax.jdo-3.2.0-m3.jar', 'javolution-5.5.1.jar', 'jaxb-api-2.2.11.jar', 'jaxb-runtime-2.3.2.jar', 'jcl-over-slf4j-2.0.7.jar', 'jdo-api-3.0.1.jar', 'jdom2-2.0.6.jar', 'jersey-client-2.40.jar', 'jersey-common-2.40.jar', 'jersey-container-servlet-2.40.jar', 'jersey-container-servlet-core-2.40.jar', 'jersey-hk2-2.40.jar', 'jersey-server-2.40.jar', 'jettison-1.1.jar', 'jetty-util-9.4.52.v20230823.jar', 'jetty-util-ajax-9.4.52.v20230823.jar', 'jline-2.14.6.jar', 'joda-time-2.12.5.jar', 'jodd-core-3.5.2.jar', 'json-1.8.jar', 'json4s-ast_2.12-3.7.0-M11.jar', 'json4s-core_2.12-3.7.0-M11.jar', 'json4s-jackson_2.12-3.7.0-M11.jar', 'json4s-scalap_2.12-3.7.0-M11.jar', 'jsr305-3.0.0.jar', 'jta-1.1.jar', 'jul-to-slf4j-2.0.7.jar', 'kryo-shaded-4.0.2.jar', 'kubernetes-client-6.7.2.jar', 'kubernetes-client-api-6.7.2.jar', 'kubernetes-httpclient-okhttp-6.7.2.jar', 'kubernetes-model-admissionregistration-6.7.2.jar', 'kubernetes-model-apiextensions-6.7.2.jar', 'kubernetes-model-apps-6.7.2.jar', 'kubernetes-model-autoscaling-6.7.2.jar', 'kubernetes-model-batch-6.7.2.jar', 'kubernetes-model-certificates-6.7.2.jar', 'kubernetes-model-common-6.7.2.jar', 'kubernetes-model-coordination-6.7.2.jar', 'kubernetes-model-core-6.7.2.jar', 'kubernetes-model-discovery-6.7.2.jar', 'kubernetes-model-events-6.7.2.jar', 'kubernetes-model-extensions-6.7.2.jar', 'kubernetes-model-flowcontrol-6.7.2.jar', 'kubernetes-model-gatewayapi-6.7.2.jar', 'kubernetes-model-metrics-6.7.2.jar', 'kubernetes-model-networking-6.7.2.jar', 'kubernetes-model-node-6.7.2.jar', 'kubernetes-model-policy-6.7.2.jar', 'kubernetes-model-rbac-6.7.2.jar', 'kubernetes-model-resource-6.7.2.jar', 'kubernetes-model-scheduling-6.7.2.jar', 'kubernetes-model-storageclass-6.7.2.jar', 'lapack-3.0.3.jar', 'leveldbjni-all-1.8.jar', 'libfb303-0.9.3.jar', 'libthrift-0.12.0.jar', 'log4j-1.2-api-2.20.0.jar', 'log4j-api-2.20.0.jar', 'log4j-core-2.20.0.jar', 'log4j-slf4j2-impl-2.20.0.jar', 'logging-interceptor-3.12.12.jar', 'lz4-java-1.8.0.jar', 'metrics-core-4.2.19.jar', 'metrics-graphite-4.2.19.jar', 'metrics-jmx-4.2.19.jar', 'metrics-json-4.2.19.jar', 'metrics-jvm-4.2.19.jar', 'minlog-1.3.0.jar', 'netty-all-4.1.96.Final.jar', 'netty-buffer-4.1.96.Final.jar', 'netty-codec-4.1.96.Final.jar', 'netty-codec-http-4.1.96.Final.jar', 'netty-codec-http2-4.1.96.Final.jar', 'netty-codec-socks-4.1.96.Final.jar', 'netty-common-4.1.96.Final.jar', 'netty-handler-4.1.96.Final.jar', 'netty-handler-proxy-4.1.96.Final.jar', 'netty-resolver-4.1.96.Final.jar', 'netty-transport-4.1.96.Final.jar', 'netty-transport-classes-epoll-4.1.96.Final.jar', 'netty-transport-classes-kqueue-4.1.96.Final.jar', 'netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar', 'netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar', 'netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar', 'netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar', 'netty-transport-native-unix-common-4.1.96.Final.jar', 'objenesis-3.3.jar', 'okhttp-3.12.12.jar', 'okio-1.15.0.jar', 'opencsv-2.3.jar', 'opentracing-api-0.33.0.jar', 'opentracing-noop-0.33.0.jar', 'opentracing-util-0.33.0.jar', 'orc-core-1.9.2-shaded-protobuf.jar', 'orc-mapreduce-1.9.2-shaded-protobuf.jar', 'orc-shims-1.9.2.jar', 'oro-2.0.8.jar', 'osgi-resource-locator-1.0.3.jar', 'paranamer-2.8.jar', 'parquet-column-1.13.1.jar', 'parquet-common-1.13.1.jar', 'parquet-encoding-1.13.1.jar', 'parquet-format-structures-1.13.1.jar', 'parquet-hadoop-1.13.1.jar', 'parquet-jackson-1.13.1.jar', 'pickle-1.3.jar', 'py4j-0.10.9.7.jar', 'rocksdbjni-8.3.2.jar', 'scala-collection-compat_2.12-2.7.0.jar', 'scala-compiler-2.12.18.jar', 'scala-library-2.12.18.jar', 'scala-parser-combinators_2.12-2.3.0.jar', 'scala-reflect-2.12.18.jar', 'scala-xml_2.12-2.1.0.jar', 'shims-0.9.45.jar', 'slf4j-api-2.0.7.jar', 'snakeyaml-2.0.jar', 'snakeyaml-engine-2.6.jar', 'snappy-java-1.1.10.3.jar', 'spark-catalyst_2.12-3.5.1.jar', 'spark-common-utils_2.12-3.5.1.jar', 'spark-core_2.12-3.5.1.jar', 'spark-graphx_2.12-3.5.1.jar', 'spark-hadoop-cloud_2.12-3.5.1.jar', 'spark-hive_2.12-3.5.1.jar', 'spark-kubernetes_2.12-3.5.1.jar', 'spark-kvstore_2.12-3.5.1.jar', 'spark-launcher_2.12-3.5.1.jar', 'spark-mllib-local_2.12-3.5.1.jar', 'spark-mllib_2.12-3.5.1.jar', 'spark-network-common_2.12-3.5.1.jar', 'spark-network-shuffle_2.12-3.5.1.jar', 'spark-repl_2.12-3.5.1.jar', 'spark-sketch_2.12-3.5.1.jar', 'spark-sql-api_2.12-3.5.1.jar', 'spark-sql_2.12-3.5.1.jar', 'spark-streaming_2.12-3.5.1.jar', 'spark-tags_2.12-3.5.1.jar', 'spark-unsafe_2.12-3.5.1.jar', 'spire-macros_2.12-0.17.0.jar', 'spire-platform_2.12-0.17.0.jar', 'spire-util_2.12-0.17.0.jar', 'spire_2.12-0.17.0.jar', 'stax-api-1.0.1.jar', 'stream-2.9.6.jar', 'threeten-extra-1.7.1.jar', 'tink-1.9.0.jar', 'transaction-api-1.1.jar', 'univocity-parsers-2.9.1.jar', 'wildfly-openssl-1.0.7.Final.jar', 'xbean-asm9-shaded-4.23.jar', 'xz-1.9.jar', 'zjsonpatch-0.3.0.jar', 'zookeeper-3.6.3.jar', 'zookeeper-jute-3.6.3.jar', 'zstd-jni-1.5.5-4.jar']
['java_opts.txt', 'spark-sql-kafka-0-10_2.12-3.5.1.jar', 'kafka-clients-2.6.1.jar', 'spark-token-provider-kafka-0-10_2.12-3.5.1.jar', 'commons-pool2-2.11.1.jar']
24/09/06 08:11:26 INFO SparkContext: Running Spark version 3.5.1
24/09/06 08:11:26 INFO SparkContext: OS info Linux, 4.18.0-477.15.1.el8_8.x86_64, amd64
24/09/06 08:11:26 INFO SparkContext: Java version 17.0.11
24/09/06 08:11:26 INFO ResourceUtils: ==============================================================
24/09/06 08:11:26 INFO ResourceUtils: No custom resources configured for spark.driver.
24/09/06 08:11:26 INFO ResourceUtils: ==============================================================
24/09/06 08:11:26 INFO SparkContext: Submitted application: File Streaming Demo
24/09/06 08:11:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/09/06 08:11:26 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
24/09/06 08:11:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/09/06 08:11:26 INFO SecurityManager: Changing view acls to: 185,harshal
24/09/06 08:11:26 INFO SecurityManager: Changing modify acls to: 185,harshal
24/09/06 08:11:26 INFO SecurityManager: Changing view acls groups to: 
24/09/06 08:11:26 INFO SecurityManager: Changing modify acls groups to: 
24/09/06 08:11:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: 185, harshal; groups with view permissions: EMPTY; users with modify permissions: 185, harshal; groups with modify permissions: EMPTY
24/09/06 08:11:27 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
24/09/06 08:11:27 INFO SparkEnv: Registering MapOutputTracker
24/09/06 08:11:27 INFO SparkEnv: Registering BlockManagerMaster
24/09/06 08:11:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/09/06 08:11:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/09/06 08:11:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/09/06 08:11:27 INFO DiskBlockManager: Created local directory at /var/data/spark-bb3dbc1e-b9d6-4164-ae7d-c6cfaade0657/blockmgr-692880d2-69d2-4b7f-9c5a-5cccde840e7d
24/09/06 08:11:27 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB
24/09/06 08:11:27 INFO SparkEnv: Registering OutputCommitCoordinator
24/09/06 08:11:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/09/06 08:11:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/09/06 08:11:27 INFO SparkContext: Added JAR local:/mounts/shared-volume/shared/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar at file:/mounts/shared-volume/shared/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1725610286628
24/09/06 08:11:27 INFO SparkContext: Added JAR local:/mounts/shared-volume/shared/jars/kafka-clients-2.6.1.jar at file:/mounts/shared-volume/shared/jars/kafka-clients-2.6.1.jar with timestamp 1725610286628
24/09/06 08:11:27 INFO SparkContext: Added JAR local:/mounts/shared-volume/shared/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:/mounts/shared-volume/shared/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1725610286628
24/09/06 08:11:27 INFO SparkContext: Added JAR local:/mounts/shared-volume/shared/jars/commons-pool2-2.11.1.jar at file:/mounts/shared-volume/shared/jars/commons-pool2-2.11.1.jar with timestamp 1725610286628
24/09/06 08:11:27 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
24/09/06 08:11:29 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.
24/09/06 08:11:29 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/09/06 08:11:29 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/09/06 08:11:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
24/09/06 08:11:29 INFO NettyBlockTransferService: Server created on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc 10.224.118.142:7079
24/09/06 08:11:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/09/06 08:11:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc, 7079, None)
24/09/06 08:11:29 INFO BlockManagerMasterEndpoint: Registering block manager pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 with 2.1 GiB RAM, BlockManagerId(driver, pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc, 7079, None)
24/09/06 08:11:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc, 7079, None)
24/09/06 08:11:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc, 7079, None)
24/09/06 08:11:29 INFO SingleEventLogFileWriter: Logging events to file:/opt/mapr/spark/sparkhs-eventlog-storage/spark-20e4c0ffd8c4425584354b12b3e64c6f.inprogress
24/09/06 08:11:45 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.224.118.233:53478
24/09/06 08:11:45 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.224.118.233:53482) with ID 1,  ResourceProfileId 0
24/09/06 08:11:45 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
24/09/06 08:11:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.224.118.233:34411 with 2.1 GiB RAM, BlockManagerId(1, 10.224.118.233, 34411, None)
24/09/06 08:11:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/09/06 08:11:46 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

root
 |-- from_json(CAST(value AS STRING)): struct (nullable = true)
 |    |-- id: integer (nullable = true)
 |    |-- name: string (nullable = true)
 |    |-- address: string (nullable = true)
 |    |-- amount: integer (nullable = true)

root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- amount: integer (nullable = true)

24/09/06 08:11:48 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/09/06 08:11:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/09/06 08:11:48 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5 resolved to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5.
24/09/06 08:11:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/09/06 08:11:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/metadata using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/.metadata.5e1f622c-6db0-44a6-aa83-b656cbbd3207.tmp
24/09/06 08:11:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/.metadata.5e1f622c-6db0-44a6-aa83-b656cbbd3207.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/metadata
24/09/06 08:11:48 INFO MicroBatchExecution: Starting [id = c80bb77a-3e69-4cee-9ded-cf46bf99a62a, runId = 4af338a3-a634-44fd-b02c-0037fa3e37e1]. Use file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5 to store the query checkpoint.
24/09/06 08:11:48 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4bdcce95] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3ea839e2]
24/09/06 08:11:48 INFO OffsetSeqLog: BatchIds found from listing: 
24/09/06 08:11:48 INFO OffsetSeqLog: BatchIds found from listing: 
24/09/06 08:11:48 INFO MicroBatchExecution: Starting new streaming query.
24/09/06 08:11:48 INFO MicroBatchExecution: Stream started from {}
24/09/06 08:11:48 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [172.31.37.92:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/09/06 08:11:48 INFO AbstractLogin: Successfully logged in.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'sasl.jaas.config' was supplied but isn't a known config.
24/09/06 08:11:48 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/09/06 08:11:48 INFO AppInfoParser: Kafka version: 2.6.1
24/09/06 08:11:48 INFO AppInfoParser: Kafka commitId: 6b2021cd52659cef
24/09/06 08:11:48 INFO AppInfoParser: Kafka startTimeMs: 1725610308822
24/09/06 08:11:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/sources/0/0 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/sources/0/.0.f2c35003-f085-497d-8068-6336b71c3e89.tmp
24/09/06 08:11:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/sources/0/.0.f2c35003-f085-497d-8068-6336b71c3e89.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/sources/0/0
24/09/06 08:11:49 INFO KafkaMicroBatchStream: Initial offsets: {"freshtopic":{"0":800}}
24/09/06 08:11:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/0 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.0.ad8214cb-d6eb-400d-bc40-01ada2333e44.tmp
24/09/06 08:11:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.0.ad8214cb-d6eb-400d-bc40-01ada2333e44.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/0
24/09/06 08:11:49 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1725610309764,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:11:50 INFO CodeGenerator: Code generated in 289.035737 ms
24/09/06 08:11:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/09/06 08:11:51 INFO SparkContext: Starting job: start at <unknown>:0
24/09/06 08:11:51 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
24/09/06 08:11:51 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
24/09/06 08:11:51 INFO DAGScheduler: Parents of final stage: List()
24/09/06 08:11:51 INFO DAGScheduler: Missing parents: List()
24/09/06 08:11:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0), which has no missing parents
24/09/06 08:11:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 22.8 KiB, free 2.1 GiB)
24/09/06 08:11:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 2.1 GiB)
24/09/06 08:11:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:11:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/09/06 08:11:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
24/09/06 08:11:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/09/06 08:11:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.224.118.233, executor 1, partition 0, PROCESS_LOCAL, 9328 bytes) 
24/09/06 08:11:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.224.118.233:34411 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:11:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3255 ms on 10.224.118.233 (executor 1) (1/1)
24/09/06 08:11:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/09/06 08:11:54 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 3.557 s
24/09/06 08:11:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/06 08:11:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/09/06 08:11:54 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 3.580263 s
24/09/06 08:11:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/09/06 08:11:54 INFO CodeGenerator: Code generated in 67.529903 ms
24/09/06 08:11:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 in memory (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:11:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.224.118.233:34411 in memory (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:11:55 INFO CodeGenerator: Code generated in 64.601778 ms
+-----+--------------------+--------------------+------+
|   id|                name|             address|amount|
+-----+--------------------+--------------------+------+
|96098|      Michael Wagner|8540 Cole Overpas...|494996|
|69797|Dr. Zachary Skinn...|PSC 4022, Box 591...|822477|
|36469|           Dana Boyd|1950 Fisher Burg ...| 58276|
|81849|       Lauren Medina|60285 Michael Isl...|584522|
|63367|      Sandra Estrada|34016 Cindy Run S...|231395|
|15051|     Kevin Dominguez|8087 David Field ...|779963|
|71116|      Larry Johnston|957 Rogers Pike\n...|838097|
|55417|    Joseph Rodriguez|026 Darren Fields...|934013|
|65628|      Bradley Hester|6008 Thompson Sum...|204258|
|91230|         Karen Lucas|3350 Thompson Gar...|139460|
| 8966|       Andrew Joseph|096 Howard Shore ...|240676|
|89079|    Ashley Patterson|83980 Fleming Spr...|402393|
|41226|    Mr. David Wagner|8600 Davis Turnpi...|678218|
|53722|      Roberto Becker|48131 Conner Ford...|751957|
|75687|      Hailey Hoffman|Unit 8570 Box 984...|312774|
|65781|      Jenna Thompson|89739 Megan Passa...|229027|
|98831|       Thomas Jensen|399 Melissa Port\...|519726|
|34501|            Wendy Wu|681 Katherine Exp...|  2233|
|45609|      Brandon Lawson|0539 Traci Shores...|613686|
|37392|        Thomas Boyer|921 Rivera View\n...|523098|
+-----+--------------------+--------------------+------+
only showing top 20 rows

24/09/06 08:11:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/09/06 08:11:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/0 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.0.4f41824c-3198-42b3-8527-ea3cfb527d52.tmp
24/09/06 08:11:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.0.4f41824c-3198-42b3-8527-ea3cfb527d52.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/0
24/09/06 08:11:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c80bb77a-3e69-4cee-9ded-cf46bf99a62a",
  "runId" : "4af338a3-a634-44fd-b02c-0037fa3e37e1",
  "name" : null,
  "timestamp" : "2024-09-06T08:11:48.496Z",
  "batchId" : 0,
  "numInputRows" : 300,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 40.3985995152168,
  "durationMs" : {
    "addBatch" : 5558,
    "commitOffsets" : 24,
    "getBatch" : 12,
    "latestOffset" : 1259,
    "queryPlanning" : 526,
    "triggerExecution" : 7425,
    "walCommit" : 30
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[freshtopic]]",
    "startOffset" : null,
    "endOffset" : {
      "freshtopic" : {
        "0" : 1100
      }
    },
    "latestOffset" : {
      "freshtopic" : {
        "0" : 1100
      }
    },
    "numInputRows" : 300,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 40.3985995152168,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@61c0123f",
    "numOutputRows" : 300
  }
}
24/09/06 08:12:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/09/06 08:13:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/1 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.1.3a6e6f8a-5388-49fe-98c9-b6b31a75b976.tmp
24/09/06 08:13:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.1.3a6e6f8a-5388-49fe-98c9-b6b31a75b976.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/1
24/09/06 08:13:26 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1725610406008,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:13:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/09/06 08:13:26 INFO SparkContext: Starting job: start at <unknown>:0
24/09/06 08:13:26 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
24/09/06 08:13:26 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
24/09/06 08:13:26 INFO DAGScheduler: Parents of final stage: List()
24/09/06 08:13:26 INFO DAGScheduler: Missing parents: List()
24/09/06 08:13:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at start at <unknown>:0), which has no missing parents
24/09/06 08:13:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.8 KiB, free 2.1 GiB)
24/09/06 08:13:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 2.1 GiB)
24/09/06 08:13:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:13:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/09/06 08:13:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
24/09/06 08:13:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/09/06 08:13:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.224.118.233, executor 1, partition 0, PROCESS_LOCAL, 9328 bytes) 
24/09/06 08:13:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.224.118.233:34411 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:13:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 556 ms on 10.224.118.233 (executor 1) (1/1)
24/09/06 08:13:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/09/06 08:13:26 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 0.562 s
24/09/06 08:13:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/06 08:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/09/06 08:13:26 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 0.564143 s
24/09/06 08:13:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
+-----+--------------------+--------------------+------+
|   id|                name|             address|amount|
+-----+--------------------+--------------------+------+
|97273|       Travis Snyder|86387 Melissa Tun...|769752|
| 7721|       Daniel Newman|2714 Steve Juncti...|613260|
|13060|Dr. Corey Rogers DVM|84989 Flores Port...| 62787|
|46511|        Jessica Bird|Unit 2729 Box 363...|889383|
| 3073|    Jennifer Collins|12756 Oscar Walk ...|839532|
|79761|      Timothy Garcia|201 Monica Stream...|865773|
|65717|      Caitlyn Rogers|1228 Ashley Circl...|289060|
|11433|    Matthew Caldwell|05400 Hughes Mano...|582529|
|47641|       Sheryl Warren|4523 Burns Port\n...|259976|
|29896|  Michael Padilla II|27756 Tiffany Cro...| 83864|
|10996|         Jim Solomon|04103 Fry Road Su...|968061|
|87571|         Amber Welch|922 Christina Tun...|413664|
|70818|        Marcus Baker|35381 Hayes Mall\...|365845|
|88117|     Jonathan Ramsey|1026 Valerie Prai...|458295|
|66666|     Michael Wallace|USCGC Cooper\nFPO...|729936|
|45483|     Madeline Murphy|284 Jared Trail\n...|734628|
|19506|        Ryan Daniels|0625 Lisa Spurs A...|560681|
|65964|   Cheyenne Gonzalez|685 Cindy Knolls ...|297496|
|22914|         Dale Miller|782 Russell Land ...|569191|
|64492|       Gina Peterson|USCGC Johnson\nFP...|123473|
+-----+--------------------+--------------------+------+
only showing top 20 rows

24/09/06 08:13:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/09/06 08:13:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/1 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.1.fadd6393-8470-446a-ab20-a9f7b55ae0f6.tmp
24/09/06 08:13:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 in memory (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:13:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.224.118.233:34411 in memory (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:13:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.1.fadd6393-8470-446a-ab20-a9f7b55ae0f6.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/1
24/09/06 08:13:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c80bb77a-3e69-4cee-9ded-cf46bf99a62a",
  "runId" : "4af338a3-a634-44fd-b02c-0037fa3e37e1",
  "name" : null,
  "timestamp" : "2024-09-06T08:13:26.000Z",
  "batchId" : 1,
  "numInputRows" : 100,
  "inputRowsPerSecond" : 1.694915254237288,
  "processedRowsPerSecond" : 137.7410468319559,
  "durationMs" : {
    "addBatch" : 642,
    "commitOffsets" : 38,
    "getBatch" : 1,
    "latestOffset" : 7,
    "queryPlanning" : 13,
    "triggerExecution" : 726,
    "walCommit" : 23
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[freshtopic]]",
    "startOffset" : {
      "freshtopic" : {
        "0" : 1100
      }
    },
    "endOffset" : {
      "freshtopic" : {
        "0" : 1200
      }
    },
    "latestOffset" : {
      "freshtopic" : {
        "0" : 1200
      }
    },
    "numInputRows" : 100,
    "inputRowsPerSecond" : 1.694915254237288,
    "processedRowsPerSecond" : 137.7410468319559,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@61c0123f",
    "numOutputRows" : 100
  }
}
24/09/06 08:14:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/09/06 08:15:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/2 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.2.9459ea92-15f0-4ed7-9dba-2d505582b12d.tmp
24/09/06 08:15:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/.2.9459ea92-15f0-4ed7-9dba-2d505582b12d.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/offsets/2
24/09/06 08:15:24 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1725610524007,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/09/06 08:15:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/09/06 08:15:24 INFO SparkContext: Starting job: start at <unknown>:0
24/09/06 08:15:24 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
24/09/06 08:15:24 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
24/09/06 08:15:24 INFO DAGScheduler: Parents of final stage: List()
24/09/06 08:15:24 INFO DAGScheduler: Missing parents: List()
24/09/06 08:15:24 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
24/09/06 08:15:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.8 KiB, free 2.1 GiB)
24/09/06 08:15:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 2.1 GiB)
24/09/06 08:15:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on pkf8-0cd56191c6631c5e-driver-svc.harshal-f34d94fc.svc:7079 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:15:24 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/09/06 08:15:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
24/09/06 08:15:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/09/06 08:15:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.224.118.233, executor 1, partition 0, PROCESS_LOCAL, 9328 bytes) 
24/09/06 08:15:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.224.118.233:34411 (size: 10.3 KiB, free: 2.1 GiB)
24/09/06 08:15:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 551 ms on 10.224.118.233 (executor 1) (1/1)
24/09/06 08:15:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/09/06 08:15:24 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 0.556 s
24/09/06 08:15:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/06 08:15:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/09/06 08:15:24 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 0.557803 s
24/09/06 08:15:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 2
-------------------------------------------
+-----+------------------+--------------------+------+
|   id|              name|             address|amount|
+-----+------------------+--------------------+------+
|26738|      James Butler|7537 Jeffrey Grov...| 27072|
|84508|      Tracy Parker|1863 Marcus Park\...|690939|
|95179|    Joshua Stewart|19369 Laura Viadu...|214029|
|98847|  Amanda Armstrong|4443 Garcia Creek...|335426|
|63638|       Daniel Tate|83109 Bell Spurs ...|289532|
|55706|Mr. Jason Stephens|3151 Joe Park\nWr...|782809|
|87999|      Keith Thomas|92184 Angela Knol...|185611|
|62091|     Olivia Sutton|4546 Stewart Hill...|626215|
|54811|Danielle Armstrong|764 Hull Ford Apt...|634271|
|69647|    Edwin Martinez|008 Soto Extensio...|903697|
|53141|      Kevin Morgan|40344 Cobb Height...|512254|
|40070|       Chad Wilson|971 Henderson Por...|402198|
|24903|  Daniel Turner MD|11903 Gonzalez Ma...|473418|
|33006|     Cynthia Lopez|494 Brown Path\nH...|564006|
|76427| Robert Sutton DDS|1114 Wayne Pike\n...|939192|
|40513|   Denise Terry MD|3705 Brett River\...| 62189|
|54989|     Jonathan Lowe|1678 Hall Rest\nN...|205331|
|23008|    Michael Nelson|904 Mark Cliffs\n...|297625|
|15137|         Tracy Orr|5426 Wendy Ramp S...|834409|
|70058|        Amanda Lee|0193 Tanner Heigh...|633859|
+-----+------------------+--------------------+------+
only showing top 20 rows

24/09/06 08:15:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/09/06 08:15:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/2 using temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.2.0a763a3b-76d0-45d7-9269-de27ee84038c.tmp
24/09/06 08:15:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/.2.0a763a3b-76d0-45d7-9269-de27ee84038c.tmp to file:/tmp/temporary-70bbccde-20ca-466f-9a08-ca73cabf94d5/commits/2
24/09/06 08:15:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c80bb77a-3e69-4cee-9ded-cf46bf99a62a",
  "runId" : "4af338a3-a634-44fd-b02c-0037fa3e37e1",
  "name" : null,
  "timestamp" : "2024-09-06T08:15:24.000Z",
  "batchId" : 2,
  "numInputRows" : 100,
  "inputRowsPerSecond" : 1.694915254237288,
  "processedRowsPerSecond" : 141.24293785310735,
  "durationMs" : {
    "addBatch" : 639,
    "commitOffsets" : 24,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 12,
    "triggerExecution" : 708,
    "walCommit" : 25
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[freshtopic]]",
    "startOffset" : {
      "freshtopic" : {
        "0" : 1200
      }
    },
    "endOffset" : {
      "freshtopic" : {
        "0" : 1300
      }
    },
    "latestOffset" : {
      "freshtopic" : {
        "0" : 1300
      }
    },
    "numInputRows" : 100,
    "inputRowsPerSecond" : 1.694915254237288,
    "processedRowsPerSecond" : 141.24293785310735,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@61c0123f",
    "numOutputRows" : 100
  }
}
24/09/06 08:16:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
